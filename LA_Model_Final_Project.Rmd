---
title: "L.A. Crime Scene Investigation: Data-Driven Detectives"
author: "
          Abhinaysai Kamineni
          Lasya Raghavendra
          Neeraj Magadum
          Aakash Hariharan 
          Amogh Ramagiri
"
date: "2024-12-02"
output:
  html_document:
    code_folding: hide
    number_sections: false
    toc: yes
    toc_depth: 3
    toc_float: yes
  pdf_document:
    toc: yes
    toc_depth: '3'
---

```{r setup, include=FALSE}
# The package "ezids" (EZ Intro to Data Science) includes a lot of the helper functions we developed for the course. 
# Some of the frequently used functions are loadPkg(), xkabledply(), xkablesummary(), uzscale(), etc.
library(ezids)
 
# some of common options (and the defaults) are: 
# include=T, eval=T, echo=T, results='hide'/'asis'/'markup',..., collapse=F, warning=T, message=T, error=T, cache=T, fig.width=6, fig.height=4, fig.dim=c(6,4) #inches, fig.align='left'/'center','right', 
# knitr::opts_chunk$set(warning = F, results = "markup", message = F)
knitr::opts_chunk$set(warning = F, results = "hide", message = F)
options(scientific=T, digits = 3) 

```

# ABSTRACT

The city of Los Angeles faces ongoing challenges in understanding and addressing the complexity of crime across its diverse neighborhoods. With thousands of recorded incidents varying by crime type, victim demographics, location, and modus operandi, law enforcement and city officials require deeper insights into the patterns and factors driving crime in specific areas.

The dataset includes detailed information on reported crimes such as crime codes, descriptions, victim age, sex, and descent, as well as crime locations, types of weapons used, and case statuses. However, identifying meaningful trends and correlations—such as how crime types vary by location, how victim demographics relate to specific crimes, and how different weapons are used in different crimes—remains a critical challenge.

Focus on Crime Analysis in Los Angeles from 2020 to present using a dataset containing detailed information on crime incidents.


# SMART QUESTIONS

1. How effectively can crime prediction models utilize historical data on the top three most common crimes in Los Angeles to forecast their distribution across high-frequency areas?

2. How can we analyze the increase in crime rates across Los Angeles neighborhoods from 2020 to 2023, using demographic factors such as the race and sex of victims to identify key patterns and potential underlying causes contributing to the rise in criminal activity?

3. How effectively can crime prediction models utilize victim descent, spatial data, and temporal patterns to forecast the likelihood and type of criminal incidents in different areas of Los Angeles?

4. How well can we use past data on holiday-season crimes in Los Angeles to predict where and how often the top three common crimes might happen in busy areas during November and December?

5. How can we use historical data to predict whether a crime is violent or non-violent, and forecast the trend of violent crime incidents over the next two years?


```{r 1, echo=T, results='markup'}
crime = read.csv("Crime_Data_from_2020_to_Present.csv")
head(crime)
```
## Date Analysis and Cleaning 0.1
```{r 2, echo=T, results='markup'}
nrow(crime)
ncol(crime)
```

The dataset contains 28 columns and 986500 rows.

1) DR_NO - Division of Records Number: Official file number.
2) Date Rptd - MM/DD/YYYY
3) DATE OCC - MM/DD/YYYY
4) TIME OCC - In 24 hour military time
5) AREA - The LAPD has 21 Community Police Stations referred to as Geographic Areas within the department. These Geographic Areas are sequentially numbered from 1-21.
6) AREA NAME - The 21 Geographic Areas or Patrol Divisions are also given a name designation that references a landmark or the surrounding community that it is responsible for.
7) Crm Cd - Indicates the crime committed. Crime Code 1 is the primary and most serious one. Crime Code 2, 3, and 4 are respectively less serious offenses. Lower crime class numbers are more serious.
8) Crm Desc - Indicates the crime description
9) Vict Age - Age of victim
10) Vict Sex - 	F : Female M : Male X : Unknown
11) Vict Descent - 	Descent Code: A - Other Asian B - Black C - Chinese D - Cambodian F - Filipino G - Guamanian H - Hispanic/Latin/Mexican I - American Indian/Alaskan Native J - Japanese K - Korean L - Laotian O - Other P - Pacific Islander S - Samoan U - Hawaiian V - Vietnamese W - White X - Unknown Z - Asian Indian
12) Weapon Desc - Defines the Weapon Used Code provided.
13) Location - Street address of crime incident rounded to the nearest hundred block to maintain anonymity.
14) LAT - Latitude
15) LON - Longtitude

```{r 3, echo=T, results='markup'}
na_count <- colSums(is.na(crime))
print("\nNA Count per Column:")
print(na_count)
```

```{r 4, echo=T, results='markup'}
new_crime <- crime[, colSums(is.na(crime)) == 0]
head(new_crime)

nrow(new_crime)
ncol(new_crime)
```
## Removing Unwanted Columns 0.2
```{r 5, echo=T, results='markup'}
# Feature Selection based on Smart Questions

cols_to_remove <- c(
   "Mocodes", "Rpt.Dist.No", "Part.1.2", 
  "Premis_cd","Premis.Desc", "Status", "Status.Desc","Cross.Street"
)

# Drop the specified columns
crime_data <- new_crime[, !(names(new_crime) %in% cols_to_remove)]


print("Data after removing unnecessary columns:")
print(names(crime_data))
```

```{r 6, echo=T, results='markup'}
# Date/Time Processing.
crime_data$Date_Rptd <- as.POSIXct(crime_data$Date.Rptd, format="%m/%d/%Y")
crime_data$Date_Occ <- as.POSIXct(crime_data$DATE.OCC, format="%m/%d/%Y")

crime_data$day_of_week <- weekdays(crime_data$Date_Occ)
crime_data$hour <- floor(crime_data$TIME.OCC/100)
crime_data$time_to_report <- as.numeric(difftime(crime_data$Date_Rptd, crime_data$Date_Occ, units="days"))
```

```{r 7, echo=T, results='markup'}
# Feature Level Engineering of Preprocessing.
crime_data$has_weapon <- ifelse(crime_data$Weapon.Desc == "STRONG-ARM (HANDS, FIST, FEET OR BODILY FORCE)", 0, 1)

crime_data$age_group <- cut(crime_data$Vict.Age, 
                           breaks=c(0, 18, 30, 50, 70, 100),
                           labels=c("Minor", "Young Adult", "Adult", "Senior", "Elderly"))

crime_data$time_period <- cut(crime_data$hour, 
                             breaks=c(0, 6, 12, 18, 24),
                             labels=c("Night", "Morning", "Afternoon", "Evening"))
```

```{r 8, echo=T, results='markup'}
# Categorical Encoding
#install.packages("caret")
library(caret)
dummy_vars <- dummyVars(" ~ AREA.NAME + Vict.Sex + Crm.Cd.Desc", data = crime_data)
categorical_encoded <- predict(dummy_vars, newdata = crime_data)
crime_data_encoded <- cbind(crime_data, categorical_encoded)
```


```{r 9, echo=T, results='markup'}
# Outlier Detection
q1_age <- quantile(crime_data$Vict.Age, 0.25)
q3_age <- quantile(crime_data$Vict.Age, 0.75)
iqr_age <- q3_age - q1_age
crime_data$age_outlier <- crime_data$Vict.Age < (q1_age - 1.5 * iqr_age) | 
                         crime_data$Vict.Age > (q3_age + 1.5 * iqr_age)

crime_data$coord_outlier <- crime_data$LAT < 33.7037 | crime_data$LAT > 34.3373 |
                           crime_data$LON < -118.6682 | crime_data$LON > -118.1553

crime_data$reporting_outlier <- crime_data$time_to_report > 365

crime_data_clean <- crime_data[!crime_data$age_outlier & 
                              !crime_data$coord_outlier & 
                              !crime_data$reporting_outlier, ]
```

## Data Cleaning 0.2
```{r 10, echo=T, results='markup'}
library(dplyr)
la_crime <- crime %>%
  filter(!is.na(Weapon.Used.Cd))

cols_to_remove <- c("Mocodes", "Rpt.Dist.No", "Part.1.2","Crm.Cd.1", 
                    "Crm.Cd.2", "Crm.Cd.3", "Crm.Cd.4",
                    "Premis.Cd", "Premis.Desc", "Status",
                    "Status.Desc", "Cross.Street")

la_crime <- la_crime %>% select(-all_of(cols_to_remove))
```


```{r 11, echo=T, results='markup'}
la_crime <- la_crime %>% rename(
  Division_NO = 'DR_NO',
  Date_Reported = 'Date.Rptd',
  Date_Occurred = 'DATE.OCC',
  Time_Occurred = 'TIME.OCC',
  Area_Code = 'AREA',
  Area_Name = 'AREA.NAME',
  Crime_Code = 'Crm.Cd',
  Crime_Description = 'Crm.Cd.Desc',
  Weapons_Used='Weapon.Used.Cd',    
  Weapons_Description='Weapon.Desc',
  Victim_Age = 'Vict.Age',
  Victim_Sex = 'Vict.Sex',
  Victim_Descent = 'Vict.Descent',
  #Crime_Code_1 = 'Crm.Cd',
  Location = 'LOCATION',
  Latitude = 'LAT',
  Longitude = 'LON'
)

la_crime <- la_crime %>%
  filter(Victim_Sex != "H")

```

```{r}
head(la_crime)
```

```{r}
la_crime$Date_Occurred <- sub(" .*", "", la_crime$Date_Occurred)
la_crime$Date_Occurred <- as.Date(la_crime$Date_Occurred, format = "%m/%d/%Y")

la_crime$Date_Reported <- sub(" .*", "", la_crime$Date_Reported)
la_crime$Date_Reported <- as.Date(la_crime$Date_Reported, format = "%m/%d/%Y")
```

```{r}
la_crime[la_crime == ""] <- NA #Changing the empty strings to null values
```

```{r}
la_crime_clean <- na.omit(la_crime) #Deleting the null values
```


```{r}
tail(la_crime_clean)
```


```{r}
# Define a list of violent and non-violent crimes
violent_crimes <- c("ASSAULT WITH DEADLY WEAPON ON POLICE OFFICER", 
                    "ASSAULT WITH DEADLY WEAPON, AGGRAVATED ASSAULT","ATTEMPTED ROBBERY", 
                    "BATTERY - SIMPLE ASSAULT", 
                    "BATTERY ON A FIREFIGHTER",
                    "BATTERY POLICE (SIMPLE)", 
                    "BATTERY WITH SEXUAL CONTACT",
                    "BEASTIALITY, CRIME AGAINST NATURE SEXUAL ASSLT WITH ANIM",
                    "CRIMINAL HOMICIDE", 
                    "RAPE, FORCIBLE", 
                    "ROBBERY","CHILD ABUSE (PHYSICAL) - SIMPLE ASSAULT", 
                    "KIDNAPPING", 
                    "SEXUAL PENETRATION W/FOREIGN OBJECT", 
                    "SEX,UNLAWFUL(INC MUTUAL CONSENT, PENETRATION W/ FRGN OBJ",
                    "INTIMATE PARTNER - AGGRAVATED ASSAULT",
                    "INTIMATE PARTNER - SIMPLE ASSAULT", "DRUNK ROLL",
                    "CHILD ABUSE (PHYSICAL) - AGGRAVATED ASSAULT",
                    "STRONG-ARM (HANDS,FIST,FEET OR BODILY FORCE)","HUMAN TRAFFICKING - COMMERCIAL SEX ACTS", 
                    "HUMAN TRAFFICKING - INVOLUNTARY SERVITUDE", "INCEST (SEXUAL ACTS BETWEEN BLOOD RELATIVES)", 
                    "INCITING A RIOT","SHOTS FIRED AT INHABITED DWELLING", 
                        "SHOTS FIRED AT MOVING VEHICLE, TRAIN OR AIRCRAFT",
                    "ORAL COPULATION","CHILD ABUSE (PHYSICAL) - SIMPLE ASSAULT","REPLICA FIREARMS(SALE,DISPLAY,MANUFACTURE OR DISTRIBUTE)","SEX OFFENDER REGISTRANT OUT OF COMPLIANCE", 
                        "SEX,UNLAWFUL(INC MUTUAL CONSENT, PENETRATION W/ FRGN OBJ)","CRM AGNST CHLD (13 OR UNDER) (14-15 & SUSP 10 YRS OLDER)","BOMB SCARE","EXTORTION","PANDERING",
                    "CRM AGNST CHLD(13 OR UNDER) (14-15 & SUSP 10 YRS OLDER)", "RAPE, ATTEMPTED",
                    "MANSLAUGHTER, NEGLIGENT", "HUMAN TRAFFICKING - INVOLUNTARY SERVITUDE",
                    "SODOMY/SEXUAL CONTACT B/W PENIS OF ONE PERS TO ANUS OTH",
                    "LEWD/LASCIVIOUS ACTS WITH CHILD","RAPE, ATTEMPTED","PIMPING","PROWLER",
                    "WEAPONS POSSESSION/BOMBING","LYNCHING", "LYNCHING - ATTEMPTED",
                    "HUMAN TRAFFICKING - COMMERCIAL SEX ACTS")

non_violent_crimes <- c("ARSON", "BIKE - STOLEN", "BURGLARY", 
                        "BURGLARY FROM VEHICLE", "SHOPLIFTING - PETTY THEFT ($950 & UNDER)", 
                        "VANDALISM - FELONY", "VANDALISM - MISDEAMEANOR", "PICKPOCKET","TELEPHONE PROPERTY - DAMAGE", "THEFT FROM MOTOR VEHICLE - ATTEMPT",
                        "THEFT FROM MOTOR VEHICLE - GRAND ($950.01 AND OVER)","BURGLARY, ATTEMPTED", 
                        "CHILD ABANDONMENT", "CHILD ABUSE (PHYSICAL) - SIMPLE ASSAULT", 
                        "CHILD ANNOYING (17YRS & UNDER)", "CHILD NEGLECT (SEE 300 W.I.C.)","RECKLESS DRIVING", 
                        "CHILD PORNOGRAPHY", "CHILD STEALING", "CONSPIRACY", "CONTEMPT OF COURT", 
                        "CONTRIBUTING", "COUNTERFEIT", "CREDIT CARDS, FRAUD USE ($950 & UNDER)", 
                        "CRIMINAL THREATS - NO WEAPON DISPLAYED", "CRM AGNST CHLD (13 OR UNDER)",  
                        "THEFT FROM MOTOR VEHICLE - PETTY ($950 & UNDER)","CRUELTY TO ANIMALS","LETTERS, LEWD  -  TELEPHONE CALLS, LEWD", "RESISTING ARREST", "ROBBERY","SHOPLIFTING - ATTEMPT", "SHOPLIFTING - PETTY THEFT ($950 & UNDER)", "SHOPLIFTING-GRAND THEFT ($950.01 & OVER)",
                        "LEWD CONDUCT", "LEWD/LASCIVIOUS ACTS WITH CHILD", "PEEPING TOM", "PICKPOCKET", 
                        "PICKPOCKET, ATTEMPT", "PURSE SNATCHING",  "PURSE SNATCHING - ATTEMPT",
                        "THEFT PLAIN - PETTY ($950 & UNDER)", "DEFRAUDING INNKEEPER/THEFT OF SERVICES, $950 & UNDER", 
                        "DEFRAUDING INNKEEPER/THEFT OF SERVICES, OVER $950.01", "DISHONEST EMPLOYEE - PETTY THEFT", 
                        "DISRUPT SCHOOL", "DISTURBING THE PEACE", "DOCUMENT FORGERY / STOLEN FELONY", 
                        "DRIVING WITHOUT OWNER CONSENT (DWOC)","EMBEZZLEMENT, GRAND THEFT ($950.01 & OVER)", 
                        "EMBEZZLEMENT, PETTY THEFT ($950 & UNDER)", "FAILURE TO YIELD", 
                        "FALSE IMPRISONMENT", "FALSE POLICE REPORT", "FIREARMS EMERGENCY PROTECTIVE ORDER (FIREARMS EPO)", "GRAND THEFT / INSURANCE FRAUD","THEFT FROM MOTOR VEHICLE - GRAND ($950.01 AND OVER)", 
                        "THEFT FROM MOTOR VEHICLE - PETTY ($950 & UNDER)", "THEFT FROM PERSON - ATTEMPT", 
                        "THEFT OF IDENTITY", "THEFT PLAIN - ATTEMPT", 
                        "THEFT-GRAND ($950.01 & OVER)EXCPT,GUNS,FOWL,LIVESTK,PROD",
                        "THEFT FROM PERSON - ATTEMPT", "PICKPOCKET","THEFT, COIN MACHINE - ATTEMPT", "THEFT, COIN MACHINE - GRAND ($950.01 & OVER)", 
                        "THEFT, COIN MACHINE - PETTY ($950 & UNDER)", "THEFT, PERSON", 
                        "THREATENING PHONE CALLS/LETTERS", "THROWING OBJECT AT MOVING VEHICLE", 
                        "TILL TAP - PETTY ($950 & UNDER)", "TRAIN WRECKING", "TRESPASSING", 
                        "UNAUTHORIZED COMPUTER ACCESS", "VANDALISM - FELONY ($400 & OVER, ALL CHURCH VANDALISMS)", 
                        "VANDALISM - MISDEAMEANOR ($399 OR UNDER)", "VEHICLE - ATTEMPT STOLEN", 
                        "VEHICLE - STOLEN", "VEHICLE, STOLEN - OTHER (MOTORIZED SCOOTERS, BIKES, ETC)", 
                        "VIOLATION OF COURT ORDER", "VIOLATION OF RESTRAINING ORDER", 
                        "VIOLATION OF TEMPORARY RESTRAINING ORDER","BURGLARY FROM VEHICLE, ATTEMPTED",
                        "OTHER MISCELLANEOUS CRIME","CONTEMPT OF COURT","BUNCO, PETTY THEFT",
                        "DISCHARGE FIREARMS/SHOTS FIRED", "STALKING","VANDALISM - MISDEAMEANOR ($399 OR UNDER)","OTHER ASSAULT","BUNCO, GRAND THEFT","INDECENT EXPOSURE","KIDNAPPING - GRAND ATTEMPT","BUNCO, ATTEMPT","CREDIT CARDS, FRAUD USE ($950 & UNDER",
                        "TILL TAP - PETTY ($950 & UNDER)","VIOLATION OF COURT ORDER",
                        "VIOLATION OF TEMPORARY RESTRAINING ORDER","VIOLATION OF RESTRAINING ORDER",
                        "BRANDISH WEAPON","THREATENING PHONE CALLS/LETTERS",
                        "CRIMINAL THREATS - NO WEAPON DISPLAYED","CHILD ANNOYING (17YRS & UNDER)",
                        "TRESPASSING", "FRAUD USE ($950 & UNDER)","	LETTERS, LEWD - TELEPHONE CALLS, LEWD", 
                        "THEFT OF IDENTITY", "THEFT, COIN MACHINE - GRAND ($950.01 & OVER)")
```

```{r}
la_crime_clean$Crime_Type <- ifelse(la_crime_clean$Crime_Description %in% violent_crimes, "Violent", 
                                           ifelse(la_crime_clean$Crime_Description %in% non_violent_crimes, "Non-Violent", "Other"))
```

```{r}
la_crime_clean
```

```{r}
crime_counts <- table(la_crime_clean$Crime_Type)

# View the total counts
print(crime_counts)

```

#Model Building
```{r}
la_crime_clean <- la_crime_clean[, !names(la_crime_clean) %in% c("Location", "Latitude", "Longitude","Weapons_Description","Crime_Description")]

```

```{r}
la_crime_clean$Crime_Type <- as.factor(la_crime_clean$Crime_Type)  # Ensuring Crime_Type is a factor (Violent/Non-Violent)
la_crime_clean$Victim_Sex <- as.factor(la_crime_clean$Victim_Sex)
la_crime_clean$Victim_Descent <- as.factor(la_crime_clean$Victim_Descent)
```


```{r}
library(caret)
trainIndex <- createDataPartition(la_crime_clean$Crime_Type, p = 0.7, list = FALSE)
train_data <- la_crime_clean[trainIndex, ]
test_data <- la_crime_clean[-trainIndex, ]

```

```{r}
# Reduce the training set size to,5% of the original data
trainIndex_reduced <- sample(1:nrow(train_data), size = 0.05 * nrow(train_data))

# Reduce the test set size to,5% of the original data
testIndex_reduced <- sample(1:nrow(test_data), size = 0.05 * nrow(test_data))

# Create smaller training and test datasets
train_data_reduced <- train_data[trainIndex_reduced, ]
test_data_reduced <- test_data[testIndex_reduced, ]

# sizes of the reduced datasets
dim(train_data_reduced)
dim(test_data_reduced)
```



```{r}
# Logistic Regression model
logit_model <- glm(Crime_Type ~ Area_Name + Victim_Age+ Victim_Sex  +  Weapons_Used,
                   family = binomial(),
                   data = train_data_reduced)


```


```{r}
# Display the summary of the model
summary(logit_model)
```


```{r}
predictions <- predict(logit_model, newdata = test_data, type = "response")

# Convert probabilities to binary class predictions (if needed)
predicted_class <- ifelse(predictions > 0.5, "Violent", "Non-Violent")
```

```{r}
# Actual labels from the test set
actual_class <- test_data$Crime_Type

# Create confusion matrix
confusion_matrix <- table(Predicted = predicted_class, Actual = actual_class)

# Print confusion matrix
print(confusion_matrix)

# Calculate accuracy
accuracy <- sum(diag(confusion_matrix)) / sum(confusion_matrix)
cat("Accuracy: ", accuracy, "\n")

```

```{r}
TN <- confusion_matrix["Non-Violent", "Non-Violent"]  # True Negative
FP <- confusion_matrix["Violent", "Non-Violent"]      # False Positive
FN <- confusion_matrix["Non-Violent", "Violent"]      # False Negative
TP <- confusion_matrix["Violent", "Violent"]

precision <- TP / (TP + FP)
cat("Precision:", precision, "\n")

recall <- TP / (TP + FN)
cat("Recall:", recall, "\n")

f1_score <- 2 * (precision * recall) / (precision + recall)
cat("F1-Score:", f1_score, "\n")


```


Interpretation:
1) High recall (98.1%) is a positive outcome because it means the model is very good at identifying violent instances. This would be importantto minimize the risk of missing a violent case

2) Precision(78.1%) denotes that the model is sometimes over-predicting violence—i.e., it might classify non-violent instances as violent, leading to false alarms.

3) Due to the class imbalances, the accuracy is getting affected, but it is still pretty good at making the predictions.

# XGB Model Q5
```{r}
trf <- train_data_reduced[, !names(train_data_reduced) %in% c("Division_NO", "Date_Reported","Time_Occurred","Date","Crime_Code")]
tdf <- test_data_reduced <- test_data_reduced[, !names(test_data_reduced) %in% c("Division_NO", "Date_Reported","Time_Occurred","Crime_Code")]

head(trf)
```
```{r}
str(trf)
```


```{r}
library(xgboost)
# Convert factors to numeric values for xgboost
trf$Area_Name <- as.numeric(trf$Area_Name)
trf$Date_Occurred <- as.numeric(trf$Date_Occurred)
trf$Victim_Sex <- as.numeric(trf$Victim_Sex)
#trf$Crime_Code <- as.numeric(trf$Crime_Code)
trf$Victim_Descent <- as.numeric(trf$Victim_Descent)
trf$Weapons_Used <- as.numeric(trf$Weapons_Used)
trf$Crime_Type <- as.numeric(trf$Crime_Type)

tdf$Area_Name <- as.numeric(tdf$Area_Name)
tdf$Date_Occurred <- as.numeric(tdf$Date_Occurred)
tdf$Victim_Sex <- as.numeric(tdf$Victim_Sex)
#tdf$Crime_Code <- as.numeric(tdf$Crime_Code)
tdf$Victim_Descent <- as.numeric(tdf$Victim_Descent)
tdf$Weapons_Used <- as.numeric(tdf$Weapons_Used)
tdf$Crime_Type <- as.numeric(tdf$Crime_Type)
```

```{r}
trainIndex2 <- createDataPartition(trf$Crime_Type, p = 0.7, list = FALSE)
trainData2 <- trf[trainIndex2, ]
testData2 <- trf[-trainIndex2, ]
```

```{r}


# Recode labels from 1, 2 to 0, 1
trainData2$Crime_Type <- ifelse(trainData2$Crime_Type == 1, 0, 1)

testData2$Crime_Type <- ifelse(testData2$Crime_Type == 1, 0, 1)

# Check the recoded labels
table(trainData2$Crime_Type)

```

```{r}
# Separate features (X) and labels (y)
X_train <- as.matrix(trainData2[, -which(names(trainData2) == "Crime_Type")])  # Features
y_train <- as.vector(trainData2$Crime_Type)  # Labels

X_test <- as.matrix(testData2[, -which(names(testData2) == "Crime_Type")])  # Features
y_test <- as.vector(testData2$Crime_Type)  # Labels

# Convert data to DMatrix format for xgboost
dtrain <- xgb.DMatrix(data = X_train, label = y_train)
dtest <- xgb.DMatrix(data = X_test, label = y_test)

```

```{r}
# Set hyperparameters for XGBoost
params <- list(
  objective = "binary:logistic",  # Binary classification
  eval_metric = "logloss",        # Log-loss metric for binary classification
  max_depth = 6,                 # Maximum depth of a tree
  eta = 0.1,                     # Learning rate
  subsample = 0.8,                # Fraction of samples used per tree
  colsample_bytree = 0.8          # Fraction of features used per tree
)

# Train the model
model <- xgb.train(
  params = params,
  data = dtrain,
  nrounds = 100,                  # Number of boosting rounds (trees)
  watchlist = list(train = dtrain, test = dtest), # Watch the training process
  early_stopping_rounds = 10       # Stop early if no improvement
)

```

```{r}
evals_result <- model$evaluation_log

# Plot training and validation errors
library(ggplot2)
ggplot(evals_result, aes(x = iter)) +
  geom_line(aes(y = train_logloss, color = "Train Error")) +
  geom_line(aes(y = test_logloss, color = "Test Error")) +
  labs(title = "Training vs Test Error", x = "Iterations", y = "Error Rate") +
  scale_color_manual(values = c("blue", "red"))
```

```{r}
# Make predictions on the test set
pred_prob <- predict(model, dtest)

# Convert probabilities to class labels (threshold = 0.5)
pred_labels <- ifelse(pred_prob > 0.5, 1, 0)

# Evaluate the performance using confusion matrix
conf_matrix <- confusionMatrix(factor(pred_labels), factor(y_test))
print(conf_matrix)

```

```{r}
precision <- conf_matrix$byClass['Precision']  # Precision
recall <- conf_matrix$byClass['Recall']        # Recall
f1_score <- conf_matrix$byClass['F1'] 

# Print the metrics
cat("Precision:", precision, "\n")
cat("Recall:", recall, "\n")
cat("F1-Score:", f1_score, "\n")
```

```{r}
importance_matrix <- xgb.importance(model = model)

# Print the importance matrix
print(importance_matrix)

# Plot the importance
xgb.plot.importance(importance_matrix)

```
Interpretation:
1) Precision (84.6%) and Recall (96.8%) are both high, which is great. The model is not only good at predicting violent instances accurately (high precision) but also doing a great job at identifying nearly all violent instances (high recall).
2) F1-Score of 90.3% shows that the model is effectively balancing precision and recall. This is very impressive because both metrics are high, indicating the model is performing well overall.


#Forecasting

```{r}
violent_data <- la_crime_clean %>% filter(Crime_Type == "Violent")
```


```{r}
library(lubridate)
# Create a new column for month and year
violent_data$YearMonth <- floor_date(violent_data$Date_Occurred, "month")

# Aggregate the number of violent crimes by YearMonth
violent_monthly <- violent_data %>%
  group_by(YearMonth) %>%
  summarise(violent_crimes = n())

# View the aggregated data
head(violent_monthly)
```
```{r}
# Convert to time series object
violent_ts <- ts(violent_monthly$violent_crimes, start = c(year(min(violent_monthly$YearMonth)), month(min(violent_monthly$YearMonth))), frequency = 12)

# Plot the time series
plot(violent_ts, main = "Monthly Violent Crimes", ylab = "Number of Violent Crimes", xlab = "Time")

```

```{r}
library(forecast)

# Fit the ARIMA model
arima_model <- auto.arima(violent_ts,seasonal = TRUE, stepwise = FALSE, approximation = FALSE)

# Print the summary of the model
summary(arima_model)

# Forecast the next 12 months (for example)
forecasted_values <- forecast(arima_model, h = 24)

# Plot the forecast
plot(forecasted_values, main = "Forecast of Violent Crimes")

```
```{r}
# You can split the time series into train and test sets if needed
train_data <- window(violent_ts, end = c(2022, 1))  # Training data until January 2023
test_data <- window(violent_ts, start = c(2022, 2))  # Test data from February 2023 onward

# Fit ARIMA on the training data
arima_model_train <- auto.arima(train_data)

# Forecast
forecast_train <- forecast(arima_model_train, h = length(test_data))

# Plot forecast vs actual
plot(forecast_train, main = "ARIMA Forecast vs Actual Violent Crimes")
lines(test_data, col = 'red')

# Calculate accuracy metrics
accuracy(forecast_train, test_data)

```
1) Indications of overfitting, where the model works decently on the training set but fails to generalize well to unseen data (the test set)



```{r}
# Apply Holt-Winters model
hw_model <- HoltWinters(violent_ts)

# Display model parameters
summary(hw_model)

```
```{r}
# Forecast for the next 12 months (or desired forecast horizon)
hw_forecast <- forecast(hw_model, h = 24)

# Plot the forecast
plot(hw_forecast)

# View the forecasted values
print(hw_forecast)

```

```{r}
# Check model accuracy
accuracy(hw_forecast)

```

The Holt-Winters model has performed reasonably well, as indicated by the MASE (0.271) and ACF1 (0.0376). The model appears to perform better than a naive forecasting approach (which would predict no change from the previous time period).

```{r}
# Convert the forecast to a data frame for easy plotting
forecast_df <- data.frame(
  Date = time(hw_forecast$mean),
  Forecasted = as.numeric(hw_forecast$mean)
)

actual_df <- data.frame(
  Date = time(violent_ts),
  Actual = as.numeric(violent_ts)
)
```

```{r}

                    
# Merge the actual and forecasted data for plotting
combined_df <- merge(actual_df, forecast_df, by = "Date", all = TRUE)

# Plot using ggplot2
library(ggplot2)

ggplot(combined_df, aes(x = Date)) +
  geom_line(aes(y = Actual, color = "Actual"), size = 1) + 
  geom_line(aes(y = Forecasted, color = "Forecasted"), size = 1, linetype = "dashed") +
  labs(title = "Actual vs Forecasted Violent Crimes (Holt-Winters)",
       x = "Date",
       y = "Number of Violent Crimes") +
  scale_color_manual(values = c("Actual" = "blue", "Forecasted" = "red")) +
  theme_minimal() +
  theme(legend.title = element_blank()) +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```



SMART Question

1. How effectively can crime prediction models utilize historical data on the top three most common crimes in Los Angeles to forecast their distribution across high-frequency areas?

```{r}
library(dplyr)
library(lubridate)
crime_data$DATE.OCC <- as.Date(crime_data$DATE.OCC, format = "%m/%d/%Y")

# Identify top 3 crimes overall from 2020 to present
top_crimes <- crime_data %>%
  filter(year(DATE.OCC) >= 2020 & year(DATE.OCC) <= 2023 ) %>%
  group_by(Crm.Cd, Crm.Cd.Desc) %>%
  summarise(count = n()) %>%
  arrange(desc(count)) %>%
  head(3)

top_crime_locations <- crime_data %>%
  filter(year(Date_Occ) >= 2020 & year(Date_Occ) <= 2023,
         Crm.Cd %in% top_crimes$Crm.Cd) %>%
  group_by(Crm.Cd, Crm.Cd.Desc, AREA.NAME) %>%
  summarise(count = n(), .groups = "drop") %>%
  arrange(Crm.Cd, desc(count))




# Analyze temporal patterns
top_crime_temporal <- crime_data %>%
  filter(year(Date_Occ) >= 2020 & year(Date_Occ) <= 2023,
         Crm.Cd %in% top_crimes$Crm.Cd) %>%
  mutate(month = month(Date_Occ),
         day_of_week = wday(Date_Occ, label = TRUE)) %>%
  group_by(Crm.Cd, Crm.Cd.Desc, month, day_of_week) %>%
  summarise(count = n(), .groups = "drop") %>%
  arrange(Crm.Cd, desc(count))


# Analyze demographic factors
top_crime_demographics <- crime_data %>%
  filter(year(Date_Occ) >= 2020 & year(Date_Occ) <= 2023,
         Crm.Cd %in% top_crimes$Crm.Cd) %>%
  group_by(Crm.Cd, Crm.Cd.Desc, Vict.Sex, Vict.Descent) %>%
  summarise(count = n(), .groups = "drop") %>%
  arrange(Crm.Cd, desc(count))
```

```{r}
top_crimes
top_crime_locations

top_crime_temporal
top_crime_demographics
```



```{r}
library(randomForest)
library(caret)
library(lubridate)
library(dplyr)

crime_data_new <- crime_data[crime_data$Vict.Age != 0, ]
crime_data_new <- crime_data_new %>% filter(LAT != 0 | LON != 0)

crime_data_rf <- crime_data_new %>%
  filter(
    Crm.Cd %in% c(510, 624, 354),
    year(Date_Occ) >= 2020 & year(Date_Occ) <=2023) %>%
  mutate(
    month = month(Date_Occ),
    day_of_week = as.factor(wday(Date_Occ)),
    AREA.NAME = as.factor(AREA.NAME),
    Vict.Sex = as.factor(Vict.Sex),
    Crm.Cd = as.factor(Crm.Cd)
  )
dim(crime_data_rf)


# Create feature matrix and target variable
X <- crime_data_rf %>% select(AREA.NAME, month, day_of_week, Vict.Sex,Vict.Age, LAT, LON)
y <- crime_data_rf$Crm.Cd

# Split the data
set.seed(42)
train_index <- createDataPartition(y, p = 0.8, list = FALSE)
X_train <- X[train_index, ]
X_test <- X[-train_index, ]
y_train <- y[train_index]
y_test <- y[-train_index]

# Train Random Forest model
rf_model <- randomForest(x = X_train, y = y_train, ntree = 500)

# Make predictions
y_pred <- predict(rf_model, X_test)

# Calculate accuracy
accuracy <- sum(y_pred == y_test) / length(y_test)
accuracy_percentage <- accuracy * 100

print(paste("Model Accuracy:", round(accuracy_percentage, 2), "%"))
```


LON 100.00
LAT 93.54
Vict.Age 68.66
AREA.NAME 31.05
month 30.53
Vict.Sex 10.00
```{r}
library(xgboost)

# Prepare data for XGBoost
# Convert categorical variables to numeric
X_train_xgb <- model.matrix(~.-1, data = X_train)
X_test_xgb <- model.matrix(~.-1, data = X_test)

# Convert target variable to numeric (0-based)
y_train_xgb <- as.numeric(y_train) - 1
y_test_xgb <- as.numeric(y_test) - 1

# Set XGBoost parameters
xgb_params <- list(
  objective = "multi:softmax",
  num_class = 3,  # number of crime classes
  eta = 0.3,
  max_depth = 6,
  min_child_weight = 1,
  subsample = 0.8,
  colsample_bytree = 0.8
)

# Train XGBoost model
xgb_model <- xgboost(
  data = X_train_xgb,
  label = y_train_xgb,
  params = xgb_params,
  nrounds = 100,
  verbose = 0
)

# Make predictions
xgb_pred <- predict(xgb_model, X_test_xgb)

# Calculate accuracy
xgb_accuracy <- sum(xgb_pred == y_test_xgb) / length(y_test_xgb)
xgb_accuracy_percentage <- xgb_accuracy * 100

print(paste("XGBoost Model Accuracy:", round(xgb_accuracy_percentage, 2), "%"))

```


