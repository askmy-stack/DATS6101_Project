---
title: "L.A. Crime Scene Investigation: Data-Driven Detectives"
author: "
          Abhinaysai Kamineni
          Lasya Raghavendra
          Neeraj Magadum
          Aakash Hariharan 
          Amogh Ramagiri
"
date: "2024-12-02"
output:
  html_document:
    code_folding: hide
    number_sections: false
    toc: yes
    toc_depth: 3
    toc_float: yes
  pdf_document:
    toc: yes
    toc_depth: '3'
---

```{r setup, include=FALSE}
# The package "ezids" (EZ Intro to Data Science) includes a lot of the helper functions we developed for the course. 
# Some of the frequently used functions are loadPkg(), xkabledply(), xkablesummary(), uzscale(), etc.



library(ezids)
 
# some of common options (and the defaults) are: 
# include=T, eval=T, echo=T, results='hide'/'asis'/'markup',..., collapse=F, warning=T, message=T, error=T, cache=T, fig.width=6, fig.height=4, fig.dim=c(6,4) #inches, fig.align='left'/'center','right', 
# knitr::opts_chunk$set(warning = F, results = "markup", message = F)
knitr::opts_chunk$set(warning = F, results = "hide", message = F)
options(scientific=T, digits = 3) 

```

# ABSTRACT

The city of Los Angeles faces ongoing challenges in understanding and addressing the complexity of crime across its diverse neighborhoods. With thousands of recorded incidents varying by crime type, victim demographics, location, and modus operandi, law enforcement and city officials require deeper insights into the patterns and factors driving crime in specific areas.

The dataset includes detailed information on reported crimes such as crime codes, descriptions, victim age, sex, and descent, as well as crime locations, types of weapons used, and case statuses. However, identifying meaningful trends and correlations—such as how crime types vary by location, how victim demographics relate to specific crimes, and how different weapons are used in different crimes—remains a critical challenge.

Focus on Crime Analysis in Los Angeles from 2020 to present using a dataset containing detailed information on crime incidents.


# SMART QUESTIONS

1. How effectively can crime prediction models utilize historical data on the top three most common crimes in Los Angeles to forecast their distribution across high-frequency areas?

2. How can we analyze the increase in crime rates across Los Angeles neighborhoods from 2020 to 2023, using demographic factors such as the race and sex of victims to identify key patterns and potential underlying causes contributing to the rise in criminal activity?

3. How effectively can crime prediction models utilize victim descent, spatial data, and temporal patterns to forecast the likelihood and type of criminal incidents in different areas of Los Angeles?

4. How well can we use past data on holiday-season crimes in Los Angeles to predict where and how often the top three common crimes might happen in busy areas during November and December?

5. How can we predict the likelihood of different types of weapons being used in crimes (such as  theft, robbery, and homicide) in Los Angeles over the next 24 months, using crime data from the last 4 years?


```{r 1, echo=T, results='markup'}
crime = read.csv("Crime_Data_from_2020_to_Present.csv")
head(crime)
```
## Date Analysis and Cleaning 0.1
```{r 2, echo=T, results='markup'}
nrow(crime)
ncol(crime)
```

The dataset contains 28 columns and 986500 rows.

1) DR_NO - Division of Records Number: Official file number.
2) Date Rptd - MM/DD/YYYY
3) DATE OCC - MM/DD/YYYY
4) TIME OCC - In 24 hour military time
5) AREA - The LAPD has 21 Community Police Stations referred to as Geographic Areas within the department. These Geographic Areas are sequentially numbered from 1-21.
6) AREA NAME - The 21 Geographic Areas or Patrol Divisions are also given a name designation that references a landmark or the surrounding community that it is responsible for.
7) Crm Cd - Indicates the crime committed. Crime Code 1 is the primary and most serious one. Crime Code 2, 3, and 4 are respectively less serious offenses. Lower crime class numbers are more serious.
8) Crm Desc - Indicates the crime description
9) Vict Age - Age of victim
10) Vict Sex - 	F : Female M : Male X : Unknown
11) Vict Descent - 	Descent Code: A - Other Asian B - Black C - Chinese D - Cambodian F - Filipino G - Guamanian H - Hispanic/Latin/Mexican I - American Indian/Alaskan Native J - Japanese K - Korean L - Laotian O - Other P - Pacific Islander S - Samoan U - Hawaiian V - Vietnamese W - White X - Unknown Z - Asian Indian
12) Weapon Desc - Defines the Weapon Used Code provided.
13) Location - Street address of crime incident rounded to the nearest hundred block to maintain anonymity.
14) LAT - Latitude
15) LON - Longtitude


```{r 3, echo=T, results='markup'}
na_count <- colSums(is.na(crime))
print("\nNA Count per Column:")
print(na_count)
```

```{r 4, echo=T, results='markup'}
new_crime <- crime[, colSums(is.na(crime)) == 0]
head(new_crime)

nrow(new_crime)
ncol(new_crime)
```
## Removing Unwanted Columns 0.2
```{r 5, echo=T, results='markup'}
# Feature Selection based on Smart Questions

cols_to_remove <- c(
   "Mocodes", "Rpt.Dist.No", "Part.1.2", 
  "Premis_cd","Premis.Desc", "Status", "Status.Desc","Cross.Street"
)

# Drop the specified columns
crime_data <- new_crime[, !(names(new_crime) %in% cols_to_remove)]


print("Data after removing unnecessary columns:")
print(names(crime_data))
```

```{r 6, echo=T, results='markup'}
# Date/Time Processing.
crime_data$Date_Rptd <- as.POSIXct(crime_data$Date.Rptd, format="%m/%d/%Y")
crime_data$Date_Occ <- as.POSIXct(crime_data$DATE.OCC, format="%m/%d/%Y")

crime_data$day_of_week <- weekdays(crime_data$Date_Occ)
crime_data$hour <- floor(crime_data$TIME.OCC/100)
crime_data$time_to_report <- as.numeric(difftime(crime_data$Date_Rptd, crime_data$Date_Occ, units="days"))
```

```{r 7, echo=T, results='markup'}
# Feature Level Engineering of Preprocessing.
crime_data$has_weapon <- ifelse(crime_data$Weapon.Desc == "STRONG-ARM (HANDS, FIST, FEET OR BODILY FORCE)", 0, 1)

crime_data$age_group <- cut(crime_data$Vict.Age, 
                           breaks=c(0, 18, 30, 50, 70, 100),
                           labels=c("Minor", "Young Adult", "Adult", "Senior", "Elderly"))

crime_data$time_period <- cut(crime_data$hour, 
                             breaks=c(0, 6, 12, 18, 24),
                             labels=c("Night", "Morning", "Afternoon", "Evening"))
```

```{r 8, echo=T, results='markup'}
# Categorical Encoding
#install.packages("caret")
library(caret)
dummy_vars <- dummyVars(" ~ AREA.NAME + Vict.Sex + Crm.Cd.Desc", data = crime_data)
categorical_encoded <- predict(dummy_vars, newdata = crime_data)
crime_data_encoded <- cbind(crime_data, categorical_encoded)
```


```{r 9, echo=T, results='markup'}
# Outlier Detection
q1_age <- quantile(crime_data$Vict.Age, 0.25)
q3_age <- quantile(crime_data$Vict.Age, 0.75)
iqr_age <- q3_age - q1_age
crime_data$age_outlier <- crime_data$Vict.Age < (q1_age - 1.5 * iqr_age) | 
                         crime_data$Vict.Age > (q3_age + 1.5 * iqr_age)

crime_data$coord_outlier <- crime_data$LAT < 33.7037 | crime_data$LAT > 34.3373 |
                           crime_data$LON < -118.6682 | crime_data$LON > -118.1553

crime_data$reporting_outlier <- crime_data$time_to_report > 365

crime_data_clean <- crime_data[!crime_data$age_outlier & 
                              !crime_data$coord_outlier & 
                              !crime_data$reporting_outlier, ]
```

## Data Cleaning 0.2
```{r 10, echo=T, results='markup'}
library(dplyr)
la_crime <- crime %>%
  filter(!is.na('Weapon.Used.Cd'))

cols_to_remove <- c("Mocodes", "Rpt.Dist.No", "Part.1.2", 
                    "Crm.Cd.2", "Crm.Cd.3", "Crm.Cd.4",
                    "Premis.Cd", "Premis.Desc", "Status",
                    "Status.Desc", "Cross.Street")

la_crime <- la_crime %>% select(-all_of(cols_to_remove))



la_crime <- la_crime %>% rename(
  Division_NO = 'DR_NO',
  Date_Reported = 'Date.Rptd',
  Date_Occurred = 'DATE.OCC',
  Time_Occurred = 'TIME.OCC',
  Area_Code = 'AREA',
  Area_Name = 'AREA.NAME',
  Crime_Code = 'Crm.Cd',
  Crime_Description = 'Crm.Cd.Desc',
  Weapons_Used='Weapon.Used.Cd',    
  Weapons_Description='Weapon.Desc',
  Victim_Age = 'Vict.Age',
  Victim_Sex = 'Vict.Sex',
  Victim_Descent = 'Vict.Descent',
  Crime_Code_1 = 'Crm.Cd',
  Location = 'LOCATION',
  Latitude = 'LAT',
  Longitude = 'LON'
)

la_crime <- la_crime %>%
  filter(Victim_Sex != "H")


```


#SMART QUESTION 2 

How can we analyze the increase in crime rates across Los Angeles neighborhoods from 2020 to 2023 to identify victim demographic patterns, across high-crime areas on a monthly basis?

```{r}
# Extract Year from Date_Occurred
la_crime$Year <- as.numeric(format(as.Date(la_crime$Date_Occurred, format="%m/%d/%Y"), "%Y"))

# Filter data from 2020 to 2023
la_crime_filtered <- la_crime %>%
  filter(Year >= 2020 & Year <= 2023)

# Group by Year, Area, Victim Sex, and Descent
crime_trends <- la_crime_filtered %>%
  group_by(Year, Area_Name, Victim_Sex, Victim_Descent) %>%
  summarise(Crime_Count = n()) %>%
  ungroup()



# Total Crime Count by Year
crime_yearly <- la_crime_filtered %>%
  group_by(Year) %>%
  summarise(Total_Crime = n())

# Plot Crime Trends Over Years
library(ggplot2)

ggplot(crime_yearly, aes(x=Year, y=Total_Crime)) +
  geom_line(color="blue", size=1.2) +
  geom_point(size=2) +
  labs(title="Total Crimes in Los Angeles (2020-2023)",
       x="Year", y="Total Crime Count") +
  theme_minimal()

```
Crime Trends (2020-2023): Total crime in Los Angeles steadily increased from 2020 to 2022, reaching a peak of over 230,000 incidents in 2022.

Slight Decline: In 2023, crime numbers saw a slight drop compared to 2022.

Visualization Insight: The line chart highlights a significant upward trend from 2020 to 2022, followed by a minor decrease in 2023.



```{r}
# Extract Month and Aggregate Monthly Crime Data
la_crime_filtered <- la_crime_filtered %>%
  mutate(Month = as.Date(cut(as.Date(Date_Occurred, format="%m/%d/%Y"), breaks = "month"))) %>%
  group_by(Area_Name, Month) %>%
  summarise(Crime_Count = n()) %>%
  ungroup()

# View Monthly Aggregated Data
head(la_crime_filtered)

```

Monthly Crime Trends in 77th Street: Crime incidents in the 77th Street area showed a declining trend from January 2020 (1305 crimes) to March 2020 (1012 crimes).

Stabilization Observed: After reaching a low of 1064 crimes in April 2020, the crime counts began to stabilize around 1076–1098 crimes in the following months.

```{r}
# Find the Top 3 Areas with the Highest Total Crime
top_areas <- la_crime_filtered %>%
  group_by(Area_Name) %>%
  summarise(Total_Crime = sum(Crime_Count)) %>%
  arrange(desc(Total_Crime)) %>%
  top_n(3, Total_Crime) %>%
  pull(Area_Name)

# Filter Data for Top 3 Areas
crime_top_10 <- la_crime_filtered %>%
  filter(Area_Name %in% top_areas)

# Print Top 3 Areas
print(top_areas)

```



```{r}
# Load Prophet Library
if (!require(prophet)) install.packages("prophet")
library(prophet)

# Forecast Monthly Trends for Each Top Crime Area
for (area in top_areas) {
  
  
  # Prepare Data for Prophet
  area_data <- crime_top_10 %>%
    filter(Area_Name == area) %>%
    select(ds = Month, y = Crime_Count)
  
  # Train Prophet Model
  model <- prophet(area_data)
  
  # Create Future Dataframe for Next 12 Months
  future <- make_future_dataframe(model, periods = 12, freq = "month")
  
  # Predict Future Crime Trends
  forecast <- predict(model, future)
  
  # Plot Forecast

}

```

```{r}
  plot(model, forecast) +
    ggtitle(paste("Monthly Crime Forecast for", area)) +
    theme_minimal()
```
Using Prophet, we forecasted monthly crime trends for top crime areas like Pacific.
 
The graph shows a clear upward trend in crime counts from 2020 to 2024, with projections exceeding 1200 incidents per month by late 2024.

The shaded area highlights the model’s confidence interval, accounting for potential variability in predictions.


```{r}
# Extract and Summarize Victim Data for Top Areas
victim_demo <- la_crime %>%
  filter(Area_Name %in% top_areas) %>%
  group_by(Area_Name, Victim_Sex, Victim_Descent) %>%
  summarise(Crime_Count = n(), .groups = "drop") %>%
  arrange(Area_Name, desc(Crime_Count))

# View Victim Demographic Summary
print(victim_demo)

# Plot Victim Sex Distribution for Top 10 Areas
ggplot(victim_demo, aes(x = Victim_Sex, y = Crime_Count, fill = Victim_Sex)) +
  geom_bar(stat = "identity", position = "dodge") +
  facet_wrap(~ Area_Name, scales = "free_y") +
  labs(title = "Crime Distribution by Victim Sex in Top 3 Areas",
       x = "Victim Sex", y = "Crime Count") +
  theme_minimal()

# Plot Victim Descent (Race) Distribution for Top 3 Areas
ggplot(victim_demo, aes(x = Victim_Descent, y = Crime_Count, fill = Victim_Descent)) +
  geom_bar(stat = "identity", position = "dodge") +
  facet_wrap(~ Area_Name, scales = "free_y") +
  labs(title = "Crime Distribution by Victim Descent in Top 3 Areas",
       x = "Victim Descent", y = "Crime Count") +
  theme_minimal()

```

```{r}
# Forecast Monthly Trends for Each Top Crime Area
if (!require(prophet)) install.packages("prophet")
library(prophet)
library(Metrics) # For RMSE, R-Squared

# Initialize empty list to store metrics
area_metrics <- data.frame(Area_Name = character(),
                           RMSE = numeric(),
                           R2 = numeric(),
                           stringsAsFactors = FALSE)

for (area in top_areas) {
  
  cat("\nForecasting Crime Trends for Area:", area, "\n")
  
  # Prepare Data for Prophet
  area_data <- crime_top_10 %>%
    filter(Area_Name == area) %>%
    select(ds = Month, y = Crime_Count)
  
  # Train Prophet Model
  model <- prophet(area_data)
  
  # Create Future Dataframe for Next 12 Months
  future <- make_future_dataframe(model, periods = 12, freq = "month")
  
  # Predict Future Crime Trends
  forecast <- predict(model, future)
  
  # Plot Forecast
  plot(model, forecast) +
    ggtitle(paste("Monthly Crime Forecast for", area)) +
    theme_minimal()
  
  # Evaluate Metrics
  actuals <- area_data$y
  predictions <- forecast$yhat[1:length(actuals)]  # Match forecast to actual data length
  
  rmse_value <- rmse(actuals, predictions)
  r2_value <- cor(actuals, predictions)^2
  
  # Append to metrics summary
  area_metrics <- rbind(area_metrics, data.frame(
    Area_Name = area,
    RMSE = rmse_value,
    R2 = r2_value
  ))
}

# Print Summary Metrics
print("Model Performance Metrics for Each Top Area:")
print(area_metrics)

```




`
```{r}
# Load Required Libraries
library(dplyr)
library(caret)
library(xgboost)

# Prepare Data for Top 5 Victim Descents
top_5_descents <- la_crime %>%
  filter(Victim_Descent %in% c("H", "W", "B", "X", "O"))

# Add Time-Based Features
top_5_descents <- top_5_descents %>%
  mutate(
    Month = as.numeric(format(as.Date(Date_Occurred, format = "%m/%d/%Y"), "%Y%m")),
    Quarter = as.numeric(format(as.Date(Date_Occurred, format = "%m/%d/%Y"), "%m")) %/% 4 + 1,
    Day_of_Week = as.numeric(weekdays(as.Date(Date_Occurred, format = "%m/%d/%Y")))
  )

# Aggregate Data
victim_data <- top_5_descents %>%
  group_by(Month, Area_Name, Victim_Descent, Quarter, Day_of_Week) %>%
  summarise(Crime_Count = n(), .groups = "drop")

# Encode Area_Name and Victim_Descent
victim_data$Area_Name <- as.numeric(as.factor(victim_data$Area_Name))
victim_data$Victim_Descent <- as.numeric(factor(victim_data$Victim_Descent, levels = c("H", "W", "B", "X", "O"))) - 1

# Split Data into Train and Test Sets
set.seed(123)
train_index <- createDataPartition(victim_data$Victim_Descent, p = 0.8, list = FALSE)
train_data <- victim_data[train_index, ]
test_data <- victim_data[-train_index, ]

# Features and Target
x_train <- as.matrix(train_data[, c("Month", "Area_Name", "Crime_Count", "Quarter", "Day_of_Week")])
x_test <- as.matrix(test_data[, c("Month", "Area_Name", "Crime_Count", "Quarter", "Day_of_Week")])
y_train <- train_data$Victim_Descent
y_test <- test_data$Victim_Descent

# Class Weights for Imbalance
class_weights <- table(y_train)
class_weights <- sum(class_weights) / (length(class_weights) * class_weights)

# Convert to XGBoost DMatrix
dtrain <- xgb.DMatrix(data = x_train, label = y_train, weight = class_weights[y_train + 1])
dtest <- xgb.DMatrix(data = x_test, label = y_test)

# Hyperparameter Tuning with Grid Search
param_grid <- expand.grid(
  max_depth = c(6, 8, 10),
  eta = c(0.05, 0.1),
  subsample = c(0.8, 1),
  colsample_bytree = c(0.8, 1),
  lambda = c(1, 2),
  alpha = c(0.5, 1)
)

best_accuracy <- 0
best_params <- NULL

for (i in 1:nrow(param_grid)) {
  params <- list(
    objective = "multi:softmax",
    num_class = 5,
    eval_metric = "mlogloss",
    max_depth = param_grid$max_depth[i],
    eta = param_grid$eta[i],
    subsample = param_grid$subsample[i],
    colsample_bytree = param_grid$colsample_bytree[i],
    lambda = param_grid$lambda[i],
    alpha = param_grid$alpha[i]
  )
  
  # Train Model
  xgb_model <- xgb.train(params = params, data = dtrain, nrounds = 200, verbose = 0)
  
  # Predict and Evaluate
  y_pred <- predict(xgb_model, dtest)
  accuracy <- mean(y_pred == y_test)
  
  if (accuracy > best_accuracy) {
    best_accuracy <- accuracy
    best_params <- params
  }
}

# Train Final Model with Best Parameters
final_model <- xgb.train(params = best_params, data = dtrain, nrounds = 200, verbose = 1)

# Predict and Evaluate
y_pred <- predict(final_model, dtest)
descents <- c("H", "W", "B", "X", "O")
y_pred_labels <- descents[y_pred + 1]
y_test_labels <- descents[y_test + 1]

# Confusion Matrix
conf_matrix <- confusionMatrix(factor(y_pred_labels), factor(y_test_labels))
print("Confusion Matrix:")
print(conf_matrix)

# Print Overall Accuracy
accuracy <- conf_matrix$overall['Accuracy']
cat("Overall Accuracy:", round(accuracy, 3), "\n")
# Feature Importance
importance_matrix <- xgb.importance(feature_names = colnames(x_train), model = final_model)

# Print Feature Importance Table
print("Feature Importance Table:")
print(importance_matrix)

# Plot Feature Importance
library(ggplot2)
ggplot(importance_matrix, aes(x = reorder(Feature, Gain), y = Gain)) +
  geom_bar(stat = "identity", fill = "skyblue") +
  coord_flip() +
  labs(title = "Feature Importance (XGBoost)", x = "Features", y = "Importance (Gain)") +
  theme_minimal()


```
```{r}
# Load Required Libraries
library(dplyr)
library(caret)
library(xgboost)

# Prepare Data for Victim Sex Prediction
victim_sex_data <- la_crime %>%
  filter(Victim_Sex %in% c("M", "F", "X")) # Only include valid sex categories

# Add Time-Based Features
victim_sex_data <- victim_sex_data %>%
  mutate(
    Month = as.numeric(format(as.Date(Date_Occurred, format = "%m/%d/%Y"), "%Y%m")),
    Quarter = as.numeric(format(as.Date(Date_Occurred, format = "%m/%d/%Y"), "%m")) %/% 4 + 1,
    Day_of_Week = as.numeric(format(as.Date(Date_Occurred, format = "%m/%d/%Y"), "%u")) # 1: Monday, 7: Sunday
  )

# Aggregate Data
victim_sex_summary <- victim_sex_data %>%
  group_by(Month, Area_Name, Victim_Sex, Quarter, Day_of_Week) %>%
  summarise(Crime_Count = n(), .groups = "drop")

# Encode Area_Name and Victim_Sex
victim_sex_summary$Area_Name <- as.numeric(as.factor(victim_sex_summary$Area_Name))
victim_sex_summary$Victim_Sex <- as.numeric(factor(victim_sex_summary$Victim_Sex, levels = c("M", "F", "X"))) - 1

# Train-Test Split
set.seed(123)
train_index <- createDataPartition(victim_sex_summary$Victim_Sex, p = 0.8, list = FALSE)
train_data <- victim_sex_summary[train_index, ]
test_data <- victim_sex_summary[-train_index, ]

# Features and Target
x_train <- as.matrix(train_data[, c("Month", "Area_Name", "Crime_Count", "Quarter", "Day_of_Week")])
x_test <- as.matrix(test_data[, c("Month", "Area_Name", "Crime_Count", "Quarter", "Day_of_Week")])
y_train <- train_data$Victim_Sex
y_test <- test_data$Victim_Sex

# Class Weights for Imbalance
class_weights <- table(y_train)
class_weights <- sum(class_weights) / (length(class_weights) * class_weights)

# Convert Data to XGBoost DMatrix
dtrain <- xgb.DMatrix(data = x_train, label = y_train, weight = class_weights[y_train + 1])
dtest <- xgb.DMatrix(data = x_test, label = y_test)

# Hyperparameter Tuning
param_grid <- expand.grid(
  max_depth = c(6, 8),
  eta = c(0.05, 0.1),
  subsample = c(0.8, 1),
  colsample_bytree = c(0.8, 1)
)

best_accuracy <- 0
best_params <- NULL

for (i in 1:nrow(param_grid)) {
  params <- list(
    objective = "multi:softmax",
    num_class = 3,
    eval_metric = "mlogloss",
    max_depth = param_grid$max_depth[i],
    eta = param_grid$eta[i],
    subsample = param_grid$subsample[i],
    colsample_bytree = param_grid$colsample_bytree[i]
  )
  
  # Train Model
  model <- xgb.train(params = params, data = dtrain, nrounds = 200, verbose = 0)
  
  # Predict and Evaluate
  y_pred <- predict(model, dtest)
  accuracy <- mean(y_pred == y_test)
  
  if (accuracy > best_accuracy) {
    best_accuracy <- accuracy
    best_params <- params
  }
}

# Train Final Model
final_model <- xgb.train(params = best_params, data = dtrain, nrounds = 500, verbose = 1)

# Predict and Evaluate Final Model
y_pred <- predict(final_model, dtest)
sex_labels <- c("M", "F", "X")
y_pred_labels <- sex_labels[y_pred + 1]
y_test_labels <- sex_labels[y_test + 1]

# Confusion Matrix
conf_matrix <- confusionMatrix(factor(y_pred_labels), factor(y_test_labels))
print("Confusion Matrix:")
print(conf_matrix)

# Print Overall Accuracy
accuracy <- conf_matrix$overall['Accuracy']
cat("Overall Accuracy:", round(accuracy, 3), "\n")

```







